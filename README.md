# Data-Pipeline-Project

Worked as a Data Engineer Trainee, building and automating ETL pipelines for a retail analytics project using AWS and big data technologies.
 
-Developed data workflows with AWS Glue (PySpark), Lambda, S3, RDS, and Redshift to clean, transform, and load large-scale retail data.
 
- Automated ETL triggers using Lambda to initiate Glue jobs on S3 data arrival, enabling real-time data processing.
 
- Loaded processed data into Amazon RDS for OLTP use cases and Amazon Redshift for OLAP analytics.
 
- Integrated Amazon SNS to send notifications for S3 data arrival, ETL job success/failure, and Redshift COPY command status, ensuring proactive monitoring.
 
-Optimized Redshift data loading by switching from Glue direct writes (~40 hours) to the Redshift COPY command, reducing load time to under 1 minute.
 
- Automated the Redshift COPY process using Lambda for fast and reliable data ingestion.
 
- Built interactive dashboards and analytics reports with Amazon QuickSight for business insights.
 
 
Delivered a capstone project that significantly improved data availability, processing speed, and operational efficiency for the client.
